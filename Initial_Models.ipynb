{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This cell imports all the required libraries for our project, including tools for data handling, audio processing, model building, and visualization. The key libraries and their roles are:\n",
    "\n",
    "* OS & File Handling: \n",
    "os and glob help manage file paths and iterate through directories.\n",
    "* PyTorch & Audio Processing:\n",
    "torch, torch.nn, torch.optim, and torch.utils.data are used for constructing and training neural network models.\n",
    "torchaudio assists in processing audio data.\n",
    "* Data Manipulation & Visualization:\n",
    "pandas and numpy are used for data manipulation and numerical operations.\n",
    "matplotlib.pyplot and seaborn are for creating visualizations.\n",
    "* Machine Learning Utilities:\n",
    "sklearn.model_selection, sklearn.metrics, sklearn.tree, sklearn.preprocessing provide tools for splitting data, evaluating models, and preprocessing features.\n",
    "* Audio Feature Extraction:\n",
    "librosa is used to extract audio features, such as MFCCs.\n",
    "* Deep Learning with TensorFlow:\n",
    "tensorflow.keras.models.Sequential and tensorflow.keras.layers.Dense are used to build and train an MLP model.\n",
    "* Progress Monitoring:\n",
    "tqdm is used to display progress bars during iterative operations.\n",
    "\n",
    "This comprehensive set of imports lays the groundwork for data pre-processing, model training, and evaluation in the subsequent cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-03-31T18:14:34.819989Z",
     "iopub.status.busy": "2025-03-31T18:14:34.819719Z",
     "iopub.status.idle": "2025-03-31T18:14:53.006937Z",
     "shell.execute_reply": "2025-03-31T18:14:53.006247Z",
     "shell.execute_reply.started": "2025-03-31T18:14:34.819969Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import torchaudio\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import LabelEncoder,StandardScaler\n",
    "import librosa\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our first and natural approch was to apply the model of Decision Trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enhanced Audio Data Loading & Feature Extraction  \n",
    "\n",
    "This code ensures **robust data validation** and **feature extraction** from audio files:  \n",
    "\n",
    "- **validate_audio_path(path)**  \n",
    "  - Checks if the specified path exists and contains .wav files.  \n",
    "  - Prints a sample of found files for verification.  \n",
    "\n",
    "- **extract_audio_features(base_path)**  \n",
    "  - Extracts MFCCs, spectral centroid, and spectral bandwidth from each .wav file.  \n",
    "  - Ensures valid feature dimensions before storing data.  \n",
    "  - Handles errors gracefully, skipping problematic files.  \n",
    "\n",
    "This approach ensures data integrity before training, reducing issues caused by missing or corrupted audio files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T18:17:42.921593Z",
     "iopub.status.busy": "2025-03-31T18:17:42.921243Z",
     "iopub.status.idle": "2025-03-31T18:19:07.774894Z",
     "shell.execute_reply": "2025-03-31T18:19:07.774059Z",
     "shell.execute_reply.started": "2025-03-31T18:17:42.921567Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing Training Data ===\n",
      "Found 1140 audio files in /kaggle/input/ravdess-speechemotionrecognition/SpeechEmotionRecognition/Train\n",
      "\n",
      "Sample files being processed:\n",
      "- 03-01-08-01-01-01-02.wav\n",
      "- 03-01-01-01-01-01-02.wav\n",
      "- 03-01-07-02-01-02-02.wav\n",
      "Successfully extracted features from 1140 files\n",
      "\n",
      "Training data shape: (912, 62) (912 samples)\n",
      "Validation data shape: (228, 62) (228 samples)\n",
      "\n",
      "Emotion Classes: ['01' '02' '03' '04' '05' '06' '07' '08']\n",
      "\n",
      "=== Training Decision Tree with Validation ===\n",
      "\n",
      "=== Best Parameters ===\n",
      "max_depth: 15\n",
      "min_samples_split: 10\n",
      "min_samples_leaf: 4\n",
      "criterion: gini\n",
      "Training Score: 0.7730\n",
      "Validation Score: 0.4123\n",
      "Final model trained on 1140 samples (training + validation)\n"
     ]
    }
   ],
   "source": [
    "def validate_audio_path(path):\n",
    "    \"\"\"Check if data path contains valid files\"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"Data path {path} does not exist\")\n",
    "        \n",
    "    wav_files = glob.glob(os.path.join(path, \"**/*.wav\"), recursive=True)\n",
    "    if not wav_files:\n",
    "        raise ValueError(f\"No .wav files found in {path}\")\n",
    "    \n",
    "    print(f\"Found {len(wav_files)} audio files in {path}\")\n",
    "    return wav_files[:3]  # Return sample files for verification\n",
    "\n",
    "def extract_audio_features(base_path):\n",
    "    \"\"\"Enhanced feature extraction with validation\"\"\"\n",
    "    lst = []\n",
    "    sample_files = validate_audio_path(base_path)\n",
    "    \n",
    "    print(\"\\nSample files being processed:\")\n",
    "    for file in sample_files:\n",
    "        print(f\"- {os.path.basename(file)}\")\n",
    "    \n",
    "    for root, dirs, files in os.walk(base_path):\n",
    "        for file in files:\n",
    "            try:\n",
    "                if not file.lower().endswith('.wav'):\n",
    "                    continue\n",
    "                    \n",
    "                file_path = os.path.join(root, file)\n",
    "                X, sample_rate = librosa.load(file_path, sr=None)\n",
    "\n",
    "                # Feature extraction\n",
    "                mfccs = np.mean(librosa.feature.mfcc(\n",
    "                    y=X, sr=sample_rate, n_mfcc=60,\n",
    "                    hop_length=int(sample_rate * 0.01),\n",
    "                    n_fft=int(sample_rate * 0.02)\n",
    "                ).T, axis=0)\n",
    "\n",
    "                spectral_centroids = np.mean(librosa.feature.spectral_centroid(y=X, sr=sample_rate))\n",
    "                spectral_bandwidth = np.mean(librosa.feature.spectral_bandwidth(y=X, sr=sample_rate))\n",
    "\n",
    "                combined_features = np.concatenate([mfccs, [spectral_centroids, spectral_bandwidth]])\n",
    "                \n",
    "                # Validate feature dimensions\n",
    "                if combined_features.shape != (62,):\n",
    "                    raise ValueError(f\"Invalid feature shape {combined_features.shape} for {file}\")\n",
    "                \n",
    "                parts = file.split('-')\n",
    "                emotion_label = parts[2] if len(parts) >= 3 else \"00\"\n",
    "                lst.append((combined_features, emotion_label))\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Skipped {file}: {str(e)}\")\n",
    "                continue\n",
    "                \n",
    "    if not lst:\n",
    "        raise ValueError(\"No valid features extracted - check audio files and processing\")\n",
    "        \n",
    "    return lst\n",
    "\n",
    "class DecisionTreeTuner:\n",
    "    def __init__(self, X_train, y_train, X_val=None, y_val=None):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_val = X_val\n",
    "        self.y_val = y_val\n",
    "        self.use_validation = X_val is not None and y_val is not None\n",
    "    \n",
    "    def tune_hyperparameters(self,\n",
    "                             max_depths=[5, 8, 10, 15, 20],\n",
    "                             min_samples_splits=[2, 5, 10],\n",
    "                             min_samples_leafs=[1, 2, 4],\n",
    "                             criteria=['gini', 'entropy']):\n",
    "        best_result = {'max_score': 0, 'best_params': {}, 'val_score': 0}\n",
    "        \n",
    "        for depth in max_depths:\n",
    "            for min_split in min_samples_splits:\n",
    "                for min_leaf in min_samples_leafs:\n",
    "                    for criterion in criteria:\n",
    "                        dtree = DecisionTreeClassifier(\n",
    "                            max_depth=depth,\n",
    "                            min_samples_split=min_split,\n",
    "                            min_samples_leaf=min_leaf,\n",
    "                            criterion=criterion,\n",
    "                            class_weight='balanced',\n",
    "                            random_state=42\n",
    "                        )\n",
    "                        \n",
    "                        # If we have a validation set, use it directly\n",
    "                        if self.use_validation:\n",
    "                            dtree.fit(self.X_train, self.y_train)\n",
    "                            val_score = dtree.score(self.X_val, self.y_val)\n",
    "                            train_score = dtree.score(self.X_train, self.y_train)\n",
    "                            mean_score = val_score  # We prioritize validation score\n",
    "                        else:\n",
    "                            # Otherwise use cross-validation on training data\n",
    "                            cv_scores = cross_val_score(dtree, self.X_train, self.y_train, cv=5)\n",
    "                            mean_score = np.mean(cv_scores)\n",
    "                            train_score = np.nan\n",
    "                            val_score = mean_score\n",
    "\n",
    "                        # Record parameters if the validation/CV score improves\n",
    "                        if mean_score > best_result['max_score']:\n",
    "                            best_result['max_score'] = mean_score\n",
    "                            best_result['train_score'] = train_score\n",
    "                            best_result['val_score'] = val_score\n",
    "                            best_result['best_params'] = {\n",
    "                                'max_depth': depth,\n",
    "                                'min_samples_split': min_split,\n",
    "                                'min_samples_leaf': min_leaf,\n",
    "                                'criterion': criterion\n",
    "                            }\n",
    "        \n",
    "        # Print best parameters\n",
    "        print(\"\\n=== Best Parameters ===\")\n",
    "        for param, value in best_result['best_params'].items():\n",
    "            print(f\"{param}: {value}\")\n",
    "        print(f\"Training Score: {best_result.get('train_score', 'N/A'):.4f}\")\n",
    "        print(f\"Validation Score: {best_result['val_score']:.4f}\")\n",
    "        \n",
    "        return best_result\n",
    "\n",
    "def validate_audio_path(path):\n",
    "    \"\"\"Check if data path contains valid files\"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"Data path {path} does not exist\")\n",
    "        \n",
    "    wav_files = glob.glob(os.path.join(path, \"**/*.wav\"), recursive=True)\n",
    "    if not wav_files:\n",
    "        raise ValueError(f\"No .wav files found in {path}\")\n",
    "    \n",
    "    print(f\"Found {len(wav_files)} audio files in {path}\")\n",
    "    return wav_files[:3]  # Return sample files for verification\n",
    "\n",
    "def extract_audio_features(base_path):\n",
    "    \"\"\"Enhanced feature extraction with validation\"\"\"\n",
    "    lst = []\n",
    "    sample_files = validate_audio_path(base_path)\n",
    "    \n",
    "    print(\"\\nSample files being processed:\")\n",
    "    for file in sample_files:\n",
    "        print(f\"- {os.path.basename(file)}\")\n",
    "    \n",
    "    for root, dirs, files in os.walk(base_path):\n",
    "        for file in files:\n",
    "            try:\n",
    "                if not file.lower().endswith('.wav'):\n",
    "                    continue\n",
    "                    \n",
    "                file_path = os.path.join(root, file)\n",
    "                X, sample_rate = librosa.load(file_path, sr=None)\n",
    "                # Feature extraction\n",
    "                mfccs = np.mean(librosa.feature.mfcc(\n",
    "                    y=X, sr=sample_rate, n_mfcc=60,\n",
    "                    hop_length=int(sample_rate * 0.01),\n",
    "                    n_fft=int(sample_rate * 0.02)\n",
    "                ).T, axis=0)\n",
    "                spectral_centroids = np.mean(librosa.feature.spectral_centroid(y=X, sr=sample_rate))\n",
    "                spectral_bandwidth = np.mean(librosa.feature.spectral_bandwidth(y=X, sr=sample_rate))\n",
    "                combined_features = np.concatenate([mfccs, [spectral_centroids, spectral_bandwidth]])\n",
    "                \n",
    "                # Validate feature dimensions\n",
    "                if combined_features.shape != (62,):\n",
    "                    raise ValueError(f\"Invalid feature shape {combined_features.shape} for {file}\")\n",
    "                \n",
    "                parts = file.split('-')\n",
    "                emotion_label = parts[2] if len(parts) >= 3 else \"00\"\n",
    "                lst.append((combined_features, emotion_label))\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Skipped {file}: {str(e)}\")\n",
    "                continue\n",
    "                \n",
    "    if not lst:\n",
    "        raise ValueError(\"No valid features extracted - check audio files and processing\")\n",
    "    \n",
    "    print(f\"Successfully extracted features from {len(lst)} files\")\n",
    "    return lst\n",
    "\n",
    "def train_decision_tree(X_train, y_train, X_val, y_val):\n",
    "    \"\"\"Train a decision tree with validation\"\"\"\n",
    "    tuner = DecisionTreeTuner(X_train, y_train, X_val, y_val)\n",
    "    best_config = tuner.tune_hyperparameters()\n",
    "    \n",
    "    best_dtree = DecisionTreeClassifier(\n",
    "        max_depth=best_config['best_params']['max_depth'],\n",
    "        min_samples_split=best_config['best_params']['min_samples_split'],\n",
    "        min_samples_leaf=best_config['best_params']['min_samples_leaf'],\n",
    "        criterion=best_config['best_params']['criterion'],\n",
    "        class_weight='balanced',\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Train on combined training and validation data for final model\n",
    "    if X_val is not None and y_val is not None:\n",
    "        X_combined = np.vstack((X_train, X_val))\n",
    "        y_combined = np.concatenate((y_train, y_val))\n",
    "        best_dtree.fit(X_combined, y_combined)\n",
    "        print(f\"Final model trained on {X_combined.shape[0]} samples (training + validation)\")\n",
    "    else:\n",
    "        best_dtree.fit(X_train, y_train)\n",
    "        print(f\"Final model trained on {X_train.shape[0]} samples\")\n",
    "    \n",
    "    return best_dtree, best_config['val_score']\n",
    "\n",
    "def evaluate_decision_tree(model, X, y, class_names, set_name=\"Test\"):\n",
    "    \"\"\"Evaluate the decision tree on any dataset\"\"\"\n",
    "    preds = model.predict(X)\n",
    "    accuracy = accuracy_score(y, preds)\n",
    "    \n",
    "    print(f\"\\n=== Decision Tree {set_name} Set Evaluation ===\")\n",
    "    print(f\"{set_name} Accuracy: {accuracy:.4f}\")\n",
    "    print(classification_report(y, preds, target_names=class_names))\n",
    "    \n",
    "    # Generate confusion matrix\n",
    "    cm = confusion_matrix(y, preds)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=class_names, \n",
    "                yticklabels=class_names)\n",
    "    plt.title(f'Decision Tree Confusion Matrix ({set_name} Set)')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "def visualize_decision_tree_performance(train_acc, val_acc):\n",
    "    \"\"\"Visualize model performance across different sets\"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sets = ['Training', 'Validation']\n",
    "    accuracies = [train_acc, val_acc]\n",
    "    \n",
    "    bars = plt.bar(sets, accuracies, color=['#3498db', '#2ecc71', '#e74c3c'])\n",
    "    \n",
    "    plt.title('Decision Tree Performance Across Different Sets')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.ylim(0, 1.0)\n",
    "    \n",
    "    # Add accuracy values on top of bars\n",
    "    for bar, acc in zip(bars, accuracies):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., \n",
    "                 bar.get_height() + 0.01, \n",
    "                 f'{acc:.4f}', \n",
    "                 ha='center', \n",
    "                 fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Use actual paths (replace with your dataset paths)\n",
    "    train_path = \"path/to/dataset\" \n",
    "\n",
    "    try:\n",
    "        # Data Processing\n",
    "        print(\"\\n=== Processing Training Data ===\")\n",
    "        train_features = extract_audio_features(train_path)\n",
    "\n",
    "        # Convert to numpy arrays with validation\n",
    "        X_all_train = np.array([f[0] for f in train_features])\n",
    "        y_all_train = np.array([f[1] for f in train_features])\n",
    "\n",
    "        # Split training data into train and validation sets\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_all_train, y_all_train, test_size=0.2, random_state=42, stratify=y_all_train\n",
    "        )\n",
    "\n",
    "        print(f\"\\nTraining data shape: {X_train.shape} ({len(X_train)} samples)\")\n",
    "        print(f\"Validation data shape: {X_val.shape} ({len(X_val)} samples)\")\n",
    "        # Validate array dimensions\n",
    "        if X_train.ndim != 2 or X_val.ndim != 2:\n",
    "            raise ValueError(f\"Invalid input dimensions - Train: {X_train.shape}, Test: {X_val.shape}\")\n",
    "\n",
    "        # Encode labels\n",
    "        le = LabelEncoder()\n",
    "        all_labels = np.concatenate([y_train, y_val])\n",
    "        le.fit(all_labels)\n",
    "        y_train_enc = le.transform(y_train)\n",
    "        y_val_enc = le.transform(y_val)\n",
    "        \n",
    "        # Get emotion class names\n",
    "        class_names = le.classes_\n",
    "        print(f\"\\nEmotion Classes: {class_names}\")\n",
    "        \n",
    "        # Scale features\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "        # Train and evaluate\n",
    "        print(\"\\n=== Training Decision Tree with Validation ===\")\n",
    "        dtree, val_acc = train_decision_tree(X_train_scaled, y_train_enc, X_val_scaled, y_val_enc)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError: {str(e)}\")\n",
    "        print(\"Check:\")\n",
    "        print(\"- Data paths exist and contain .wav files\")\n",
    "        print(\"- Feature extraction produces 62-dimensional vectors\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After Decision Tree having accuracy of 42%, We used Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T18:15:10.841717Z",
     "iopub.status.busy": "2025-03-31T18:15:10.841375Z",
     "iopub.status.idle": "2025-03-31T18:17:20.488825Z",
     "shell.execute_reply": "2025-03-31T18:17:20.487884Z",
     "shell.execute_reply.started": "2025-03-31T18:15:10.841691Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing Train Data ===\n",
      "Found 1140 audio files in /kaggle/input/ravdess-speechemotionrecognition/SpeechEmotionRecognition/Train\n",
      "\n",
      "Sample files being processed:\n",
      "- 03-01-08-01-01-01-02.wav\n",
      "- 03-01-01-01-01-01-02.wav\n",
      "- 03-01-07-02-01-02-02.wav\n",
      "Successfully extracted features from 1140 files\n",
      "Validation Accuracy through Random Forest Classifier: 0.64\n"
     ]
    }
   ],
   "source": [
    "train_path = \"path/to/dataset\" \n",
    "\n",
    "print(\"\\n=== Processing Train Data ===\")\n",
    "train_features = extract_audio_features(train_path)\n",
    "\n",
    "# Convert to numpy arrays with validation\n",
    "X_all_train = np.array([f[0] for f in train_features])\n",
    "y_all_train = np.array([f[1] for f in train_features])\n",
    "\n",
    "# Split training data into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_all_train, y_all_train, test_size=0.2, random_state=42, stratify=y_all_train\n",
    ")\n",
    "\n",
    "rf_model = RandomForestClassifier(criterion=\"gini\", max_features= \"sqrt\" , n_estimators = 5000)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Compute Validation Accuracy\n",
    "y_val_pred = rf_model.predict(X_val)\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "print(f\"Validation Accuracy through Random Forest Classifier: {val_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When we evaluated the Random Forest Model, we got Validation accuracy of 64%, which is much lower and can be improved. Hence to further explore other model, we decided to use MLP\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP Classifier for Speech Emotion Recognition  \n",
    "\n",
    "This script implements a **Multi-Layer Perceptron (MLP)** to classify emotions from speech audio data using **MFCC features**.  \n",
    "\n",
    "#### Workflow:  \n",
    "1. **Extract Features**:  \n",
    "   - Load audio files from the dataset.  \n",
    "   - Compute **40 MFCCs** and take their mean across time frames.  \n",
    "\n",
    "2. **Data Preparation**:  \n",
    "   - Organize extracted features into numpy arrays.  \n",
    "   - Encode emotion labels and split the dataset into **training (80%)** and **validation (20%)** sets.  \n",
    "\n",
    "3. **Model Definition & Training**:  \n",
    "   - Define an **MLP architecture** with two hidden layers (128 and 256 neurons).  \n",
    "   - Train the model using the **Adam optimizer** and **categorical cross-entropy loss**.  \n",
    "\n",
    "4. **Performance Evaluation**:  \n",
    "   - Track **validation loss** and **accuracy**.  \n",
    "   - Report accuracy at the **epoch with minimum validation loss**, ensuring optimal model selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T18:19:51.723737Z",
     "iopub.status.busy": "2025-03-31T18:19:51.723398Z",
     "iopub.status.idle": "2025-03-31T18:20:21.531008Z",
     "shell.execute_reply": "2025-03-31T18:20:21.530275Z",
     "shell.execute_reply.started": "2025-03-31T18:19:51.723712Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 82ms/step - accuracy: 0.1434 - loss: 28.0480 - val_accuracy: 0.1886 - val_loss: 3.6110\n",
      "Epoch 2/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.1731 - loss: 3.9692 - val_accuracy: 0.1623 - val_loss: 3.7718\n",
      "Epoch 3/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.2059 - loss: 3.2004 - val_accuracy: 0.1711 - val_loss: 4.5083\n",
      "Epoch 4/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.2165 - loss: 3.3016 - val_accuracy: 0.1535 - val_loss: 4.5305\n",
      "Epoch 5/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.2381 - loss: 3.2316 - val_accuracy: 0.2412 - val_loss: 2.7944\n",
      "Epoch 6/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.2455 - loss: 2.8095 - val_accuracy: 0.2368 - val_loss: 2.8456\n",
      "Epoch 7/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.2936 - loss: 3.1226 - val_accuracy: 0.2368 - val_loss: 3.5341\n",
      "Epoch 8/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.2516 - loss: 3.1036 - val_accuracy: 0.2105 - val_loss: 2.9087\n",
      "Epoch 9/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.2794 - loss: 2.6610 - val_accuracy: 0.3772 - val_loss: 1.8012\n",
      "Epoch 10/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3428 - loss: 1.9684 - val_accuracy: 0.2061 - val_loss: 2.6727\n",
      "Epoch 11/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3416 - loss: 2.3034 - val_accuracy: 0.2325 - val_loss: 3.8064\n",
      "Epoch 12/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.2616 - loss: 2.9036 - val_accuracy: 0.1974 - val_loss: 3.5380\n",
      "Epoch 13/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.2844 - loss: 2.6812 - val_accuracy: 0.2982 - val_loss: 2.4572\n",
      "Epoch 14/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3475 - loss: 2.2400 - val_accuracy: 0.3333 - val_loss: 1.9754\n",
      "Epoch 15/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3465 - loss: 2.1721 - val_accuracy: 0.2105 - val_loss: 2.6486\n",
      "Epoch 16/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3643 - loss: 2.2237 - val_accuracy: 0.2544 - val_loss: 2.7594\n",
      "Epoch 17/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3087 - loss: 2.3411 - val_accuracy: 0.2149 - val_loss: 2.8172\n",
      "Epoch 18/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.2860 - loss: 2.7652 - val_accuracy: 0.2939 - val_loss: 2.5823\n",
      "Epoch 19/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3844 - loss: 2.1483 - val_accuracy: 0.2895 - val_loss: 3.0167\n",
      "Epoch 20/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3291 - loss: 2.5949 - val_accuracy: 0.2982 - val_loss: 2.9661\n",
      "Epoch 21/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3186 - loss: 2.7882 - val_accuracy: 0.3289 - val_loss: 2.0915\n",
      "Epoch 22/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4096 - loss: 1.8780 - val_accuracy: 0.3158 - val_loss: 2.2531\n",
      "Epoch 23/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3856 - loss: 2.0987 - val_accuracy: 0.3991 - val_loss: 1.7477\n",
      "Epoch 24/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3811 - loss: 1.8365 - val_accuracy: 0.3509 - val_loss: 1.9238\n",
      "Epoch 25/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4301 - loss: 1.8387 - val_accuracy: 0.3114 - val_loss: 1.9102\n",
      "Epoch 26/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3765 - loss: 2.1008 - val_accuracy: 0.3465 - val_loss: 2.5194\n",
      "Epoch 27/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3989 - loss: 2.0154 - val_accuracy: 0.4035 - val_loss: 2.0674\n",
      "Epoch 28/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4416 - loss: 1.8223 - val_accuracy: 0.2939 - val_loss: 2.2844\n",
      "Epoch 29/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3845 - loss: 1.9077 - val_accuracy: 0.3728 - val_loss: 1.8208\n",
      "Epoch 30/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3843 - loss: 1.8616 - val_accuracy: 0.3114 - val_loss: 2.2240\n",
      "Epoch 31/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3965 - loss: 1.8262 - val_accuracy: 0.3772 - val_loss: 1.7074\n",
      "Epoch 32/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4401 - loss: 1.5702 - val_accuracy: 0.3509 - val_loss: 1.8610\n",
      "Epoch 33/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3802 - loss: 1.8752 - val_accuracy: 0.3509 - val_loss: 2.1832\n",
      "Epoch 34/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4413 - loss: 1.8762 - val_accuracy: 0.3421 - val_loss: 2.2685\n",
      "Epoch 35/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3766 - loss: 2.4432 - val_accuracy: 0.4167 - val_loss: 3.4179\n",
      "Epoch 36/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4295 - loss: 2.1331 - val_accuracy: 0.3465 - val_loss: 2.3185\n",
      "Epoch 37/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3888 - loss: 2.1245 - val_accuracy: 0.3158 - val_loss: 2.6490\n",
      "Epoch 38/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4230 - loss: 2.0737 - val_accuracy: 0.3158 - val_loss: 2.4198\n",
      "Epoch 39/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4426 - loss: 1.7741 - val_accuracy: 0.3684 - val_loss: 2.3785\n",
      "Epoch 40/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4288 - loss: 2.2315 - val_accuracy: 0.3026 - val_loss: 2.2901\n",
      "Epoch 41/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4510 - loss: 1.6506 - val_accuracy: 0.4649 - val_loss: 1.5935\n",
      "Epoch 42/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4460 - loss: 1.6719 - val_accuracy: 0.3860 - val_loss: 1.9121\n",
      "Epoch 43/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4779 - loss: 1.6019 - val_accuracy: 0.3860 - val_loss: 2.2946\n",
      "Epoch 44/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4331 - loss: 1.9742 - val_accuracy: 0.3202 - val_loss: 2.1911\n",
      "Epoch 45/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4687 - loss: 1.7051 - val_accuracy: 0.3947 - val_loss: 2.1643\n",
      "Epoch 46/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4509 - loss: 1.9208 - val_accuracy: 0.3596 - val_loss: 2.1637\n",
      "Epoch 47/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4202 - loss: 1.7910 - val_accuracy: 0.3728 - val_loss: 1.9921\n",
      "Epoch 48/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4714 - loss: 1.5906 - val_accuracy: 0.3465 - val_loss: 1.9251\n",
      "Epoch 49/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4523 - loss: 1.5468 - val_accuracy: 0.4079 - val_loss: 1.7346\n",
      "Epoch 50/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4688 - loss: 1.4916 - val_accuracy: 0.4781 - val_loss: 1.5078\n",
      "Epoch 51/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4993 - loss: 1.3686 - val_accuracy: 0.4298 - val_loss: 2.2971\n",
      "Epoch 52/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4876 - loss: 1.6753 - val_accuracy: 0.3947 - val_loss: 1.8944\n",
      "Epoch 53/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4393 - loss: 1.8457 - val_accuracy: 0.3728 - val_loss: 2.1185\n",
      "Epoch 54/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4451 - loss: 1.8478 - val_accuracy: 0.3596 - val_loss: 1.9187\n",
      "Epoch 55/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4712 - loss: 1.6158 - val_accuracy: 0.4693 - val_loss: 1.5390\n",
      "Epoch 56/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4859 - loss: 1.4343 - val_accuracy: 0.4167 - val_loss: 1.6748\n",
      "Epoch 57/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5041 - loss: 1.4570 - val_accuracy: 0.4298 - val_loss: 1.5199\n",
      "Epoch 58/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4858 - loss: 1.4467 - val_accuracy: 0.3991 - val_loss: 1.6936\n",
      "Epoch 59/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5051 - loss: 1.3267 - val_accuracy: 0.3377 - val_loss: 2.1861\n",
      "Epoch 60/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4546 - loss: 1.6819 - val_accuracy: 0.3640 - val_loss: 1.9331\n",
      "Epoch 61/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4969 - loss: 1.3891 - val_accuracy: 0.4123 - val_loss: 1.7897\n",
      "Epoch 62/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5412 - loss: 1.2840 - val_accuracy: 0.3991 - val_loss: 1.7603\n",
      "Epoch 63/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4828 - loss: 1.5860 - val_accuracy: 0.4561 - val_loss: 1.5724\n",
      "Epoch 64/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5027 - loss: 1.5164 - val_accuracy: 0.3684 - val_loss: 1.8800\n",
      "Epoch 65/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4746 - loss: 1.5583 - val_accuracy: 0.4123 - val_loss: 1.7283\n",
      "Epoch 66/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5010 - loss: 1.4677 - val_accuracy: 0.4035 - val_loss: 1.7753\n",
      "Epoch 67/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4995 - loss: 1.3864 - val_accuracy: 0.4430 - val_loss: 1.6065\n",
      "Epoch 68/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4594 - loss: 1.5645 - val_accuracy: 0.3772 - val_loss: 1.8679\n",
      "Epoch 69/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5698 - loss: 1.2280 - val_accuracy: 0.4386 - val_loss: 1.7392\n",
      "Epoch 70/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5015 - loss: 1.3913 - val_accuracy: 0.4298 - val_loss: 2.0003\n",
      "Epoch 71/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5102 - loss: 1.4981 - val_accuracy: 0.3596 - val_loss: 2.6264\n",
      "Epoch 72/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4518 - loss: 1.8176 - val_accuracy: 0.4298 - val_loss: 1.8148\n",
      "Epoch 73/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4819 - loss: 1.6597 - val_accuracy: 0.4605 - val_loss: 1.8148\n",
      "Epoch 74/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5380 - loss: 1.3732 - val_accuracy: 0.3991 - val_loss: 1.9389\n",
      "Epoch 75/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4800 - loss: 1.3954 - val_accuracy: 0.4167 - val_loss: 1.9079\n",
      "Epoch 76/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5336 - loss: 1.4863 - val_accuracy: 0.3947 - val_loss: 1.8629\n",
      "Epoch 77/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5382 - loss: 1.4346 - val_accuracy: 0.3728 - val_loss: 2.6830\n",
      "Epoch 78/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4636 - loss: 1.8132 - val_accuracy: 0.4386 - val_loss: 1.8538\n",
      "Epoch 79/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5064 - loss: 1.3962 - val_accuracy: 0.3728 - val_loss: 1.7459\n",
      "Epoch 80/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5144 - loss: 1.3526 - val_accuracy: 0.4649 - val_loss: 1.5765\n",
      "Epoch 81/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5663 - loss: 1.3420 - val_accuracy: 0.4649 - val_loss: 1.6147\n",
      "Epoch 82/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5351 - loss: 1.3426 - val_accuracy: 0.3991 - val_loss: 1.6837\n",
      "Epoch 83/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5097 - loss: 1.3021 - val_accuracy: 0.3991 - val_loss: 1.6922\n",
      "Epoch 84/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5310 - loss: 1.3189 - val_accuracy: 0.4254 - val_loss: 1.9390\n",
      "Epoch 85/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4925 - loss: 1.5200 - val_accuracy: 0.4693 - val_loss: 1.5380\n",
      "Epoch 86/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5680 - loss: 1.3162 - val_accuracy: 0.4079 - val_loss: 2.0137\n",
      "Epoch 87/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4911 - loss: 1.4620 - val_accuracy: 0.3816 - val_loss: 1.9515\n",
      "Epoch 88/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5016 - loss: 1.4139 - val_accuracy: 0.4912 - val_loss: 1.5599\n",
      "Epoch 89/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5166 - loss: 1.3457 - val_accuracy: 0.4868 - val_loss: 1.5666\n",
      "Epoch 90/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5761 - loss: 1.2074 - val_accuracy: 0.3640 - val_loss: 1.8996\n",
      "Epoch 91/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5469 - loss: 1.3397 - val_accuracy: 0.4474 - val_loss: 1.7210\n",
      "Epoch 92/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5136 - loss: 1.3959 - val_accuracy: 0.4781 - val_loss: 1.5530\n",
      "Epoch 93/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5249 - loss: 1.4335 - val_accuracy: 0.4079 - val_loss: 1.8282\n",
      "Epoch 94/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5573 - loss: 1.2108 - val_accuracy: 0.4693 - val_loss: 1.5062\n",
      "Epoch 95/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5797 - loss: 1.1835 - val_accuracy: 0.4561 - val_loss: 1.7146\n",
      "Epoch 96/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5850 - loss: 1.2536 - val_accuracy: 0.4825 - val_loss: 1.5586\n",
      "Epoch 97/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5543 - loss: 1.2373 - val_accuracy: 0.4518 - val_loss: 1.9107\n",
      "Epoch 98/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5389 - loss: 1.4053 - val_accuracy: 0.4825 - val_loss: 1.5041\n",
      "Epoch 99/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6006 - loss: 1.0955 - val_accuracy: 0.4649 - val_loss: 1.6547\n",
      "Epoch 100/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6134 - loss: 1.1024 - val_accuracy: 0.4956 - val_loss: 1.4118\n",
      "Validation Accuracy at minimum val loss (1.4118): 0.4956\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define dataset path for training (adjust this path if needed)\n",
    "mlp_train_dataset = 'path/to/dataset'\n",
    "\n",
    "# Feature extraction function using MFCCs\n",
    "def extract_features(file_path):\n",
    "    y, sr = librosa.load(file_path, sr=None)\n",
    "    # Extract 40 MFCCs and take the mean across time frames\n",
    "    return np.mean(librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40), axis=1)\n",
    "\n",
    "# --- Prepare training dataset ---\n",
    "mlp_data, mlp_labels = [], []\n",
    "# Iterate over actor directories in the training dataset\n",
    "for actor in sorted(os.listdir(mlp_train_dataset)):\n",
    "    actor_path = os.path.join(mlp_train_dataset, actor)\n",
    "    if os.path.isdir(actor_path):\n",
    "        for file in os.listdir(actor_path):\n",
    "            file_path = os.path.join(actor_path, file)\n",
    "            # Extract features and label (using the 3rd element from the filename)\n",
    "            mlp_data.append(extract_features(file_path))\n",
    "            mlp_labels.append(file.split('-')[2])\n",
    "\n",
    "X_mlp = np.array(mlp_data)\n",
    "y_mlp = LabelEncoder().fit_transform(mlp_labels)\n",
    "\n",
    "# Split training data into train and validation sets (80:20 split)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_mlp, y_mlp, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# --- Define and compile the MLP model ---\n",
    "mlp_model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dense(len(np.unique(y_train)), activation='softmax')\n",
    "])\n",
    "\n",
    "mlp_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = mlp_model.fit(\n",
    "    X_train, y_train, \n",
    "    epochs=100, \n",
    "    batch_size=32, \n",
    "    validation_data=(X_val, y_val)\n",
    ")\n",
    "\n",
    "min_loss_idx = np.argmin(history.history['val_loss'])\n",
    "min_val_loss = history.history['val_loss'][min_loss_idx]\n",
    "best_val_acc = history.history['val_accuracy'][min_loss_idx]\n",
    "print(f'Validation Accuracy at minimum val loss ({min_val_loss:.4f}): {best_val_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our final approach was using CNN model, after an accuracy of 50% on MLP model which was less than Random Forest Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The final CNN model is in another notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6990059,
     "sourceId": 11196226,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
